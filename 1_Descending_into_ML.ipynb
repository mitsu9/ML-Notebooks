{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このページでは機械学習の基本用語を説明します。\n",
    "その後、一番簡単な例を用いて機械学習の流れについて説明します。\n",
    "\n",
    "## Framing: Key ML Terminology\n",
    "https://developers.google.com/machine-learning/crash-course/framing/ml-terminology\n",
    "\n",
    "## 基本用語\n",
    "\n",
    "- Features : 入力データ。x軸。複数あっても良い（それだけ次元が増える）。\n",
    "- Label : 出力データ。y軸。予想したいもの\n",
    "- Example : 具体的なデータ。Labelの有無で以下の2種類に分けられる。\n",
    "  - Labeled Example : FeatureとLabelのセット。これを使ってモデルに学習させる。\n",
    "  - Unlabeled Example : Labelのついていないexample\n",
    "- Model : featuresとlabelの関係\n",
    "\n",
    "モデルのライフサイクル\n",
    "\n",
    "- Training : モデルに学習させる\n",
    "- Inference : 学習されたモデルを利用してunlabeled examplesにラベル付けをおこなう\n",
    "\n",
    "モデルの種類\n",
    "\n",
    "- regression model : 連続値を扱う。身長→体重。文言→広告のクリック率\n",
    "- classification model : 離散値を扱う。メール→スパム or not。画像→犬, 猫, or ハムスター"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "https://developers.google.com/machine-learning/crash-course/descending-into-ml/linear-regression\n",
    "\n",
    "ここからは簡単な機械学習の例を用いて、機械学習の流れを説明する。\n",
    "\n",
    "ここで行いたいことは、図１にある赤い点に対して、青い線を学習することである。\n",
    "\n",
    "<img src=\"assets/linear_regression.png\">\n",
    "<図1:線形回帰>\n",
    "\n",
    "この直線は以下のように表すことができる。(一般的な一次関数の式)\n",
    "\n",
    "$$y = mx + b.$$\n",
    "\n",
    "これを機械学習の世界ではよく以下のように書く。\n",
    "\n",
    "$$ y' = b + w_1 x_1 $$\n",
    "\n",
    "where\n",
    "- $y'$ : 予想されるラベル\n",
    "- $b$ : bias\n",
    "- $w_1$ : feature 1のweight\n",
    "- $x_1$ : feature 1の値\n",
    "\n",
    "この式は高次元に拡張することもできる。\n",
    "例えば3つのfeaturesからラベル付けを行う場合、以下の式のようになる。\n",
    "\n",
    "$$ y' = b + w_1 x_1 + w_2 x_2 + w_3 x_3. $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Loss\n",
    "https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss\n",
    "\n",
    "モデルに学習させることとは、先ほどの式におけるバイアスとウェイトについて、良い値を求めるということになる。\n",
    "\n",
    "この良い値というのは、Lossが最小化されるということである。\n",
    "このような方法?考え方?をEmpirical risk minimization (ERM)と呼ぶ。\n",
    "\n",
    "Lossとはある一つのexampleに対して、モデルの予想がどれだけ悪いかを示す値である。\n",
    "図2は赤い矢印がロスを示しており、この矢印の長さがロスの大きさとなる。\n",
    "図2の右の予想の方がロスが小さいので、右の方が良いと言える。\n",
    "\n",
    "<img src=\"assets/loss.png\">\n",
    "<図2:ロス関数>\n",
    "\n",
    "あるexampleに対するロスを計算するための関数には、squared loss（aka $L_2$ loss, 二乗損失)を使う。\n",
    "\n",
    "$$ squared loss = (y - y')^2 $$\n",
    "\n",
    "二乗損失を用いてモデルの良さを求める際には、Mean square error (MSE, 平均二乗誤差) を利用する。\n",
    "これは全てのexamplesに対して二乗損失を求め、その平均を取るというものである。\n",
    "\n",
    "$$ MSE = \\frac{1}{N} \\sum_{(x,y) \\in D}(y - prediction(x))^2 $$\n",
    "\n",
    "where \n",
    "- $(x,y)$ : あるexample\n",
    "- $prediction(x)$ : モデルによって計算されるxに対するy'の値\n",
    "- $D$ : データセット\n",
    "- $N$ : $D$のサイズ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent 最急降下法\n",
    "https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent\n",
    "\n",
    "それではMSEが最小になるようなウェイトをどのように求めれば良いか。\n",
    "全てのウェイトでMSEの値を計算すれば最小は求まりまるが非常に非効率である。\n",
    "\n",
    "ここで、MSEの値をグラフ化すると必ず次の図のように凸関数になる。\n",
    "\n",
    "<img src=\"assets/mse-func.png\">\n",
    "<図3 : MSE(ロス関数)>\n",
    "\n",
    "したがって、この凸関数の極値を求めればロスを一番小さくするウェイトの値がわかる。\n",
    "\n",
    "そこで、ある初期値から少しずつウェイトを動かしていき極値を求める方法を考える。\n",
    "\n",
    "<img src=\"assets/gradient-descent.png\">\n",
    "<図4 : 最急降下法>\n",
    "\n",
    "この時、図のようにグラフが可視化されていればウェイトを増加させれば良いか減少させれば良いかがわかる。\n",
    "しかし、実際にはある一点しかわかっていないのでどちらに動かせばロスが小さくなるかがわからない。\n",
    "\n",
    "そこで、ロスが小さくなる方向を調べるために、傾きを求める=微分する。\n",
    "\n",
    "図は2次元のためウェイトを増加もしくは減少のどちらかを選択する問題になるが、高次元の場合にはロスが小さくなる方向が複数方向ある可能性があり、どの方向に動かせば良いかを決めることは難しい。\n",
    "そこで、「傾きが一番急な方向に動かす」というのが最急降下法である。\n",
    "\n",
    "このようにして傾きが一番急な方向にウェイトを更新し、更新処理を複数回繰り返すことで最終的には傾きがゼロの地点にたどり着き、極値が求められる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate\n",
    "\n",
    "learning rate（もしくはstep size)とは、次の点を決めるために使われる係数のことである。\n",
    "\n",
    "先ほど傾き(微分)を見て次にどちらに移動するかを決めると説明した。\n",
    "その時にどれだけ移動するかを$ (微分値) x learning rate $によって求める。\n",
    "\n",
    "この時、learning rateが小さければなかなか極値にたどり着かず、収束までに時間がかかる。\n",
    "\n",
    "<img src=\"assets/small_learning_rate.png\">\n",
    "\n",
    "一方で、learning rateが大きすぎると極値を通り過ぎてしまう。\n",
    "\n",
    "<img src=\"assets/large_learning_rate.png\">\n",
    "\n",
    "したがって、ちょうど良いlearning rateを設定する必要がある。\n",
    "\n",
    "機械学習において、learning rateのようなプログラマが調節する必要のあるパラメータをHyperparametersと呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下は細かい数学の部分が気になる人用のメモ\n",
    "\n",
    "最急降下法では、微分して傾きを求め、一番傾きが急な方向にウェイトを変更するが、実際その傾きはどのようにして求めるのだろう。\n",
    "\n",
    "ここで、ロス関数は以下の式で表される。\n",
    "（上のグラフでは2次元にするために$b$を無視して$w$だけを変数として書いている。本来、ここでの変数は$w, b$の二つのはず。)\n",
    "\n",
    "$$ f(w, b) = \\frac{1}{N} \\sum_{(x,y) \\in D} (y - (b + wx))^2. $$\n",
    "\n",
    "このグラフにおける極値を求めることが、ロスを最小化する$w,b$を求められることになる。\n",
    "\n",
    "2変数関数において傾きを考えることはそれぞれの変数について偏微分することになる。\n",
    "偏微分した際の値は傾きを示す。\n",
    "\n",
    "ここで、傾きが負→増加（右に移動）、傾きが正→減少(左に移動)と傾きの符号と逆向きに移動することと、移動する際にlearning rateが乗算されることを考えると、$w,b$は以下の式を用いて更新される。\n",
    "\n",
    "$$ w' = w - \\alpha \\frac{\\partial }{\\partial w} f(w, b). $$\n",
    "$$ b' = b - \\alpha \\frac{\\partial }{\\partial b} f(w, b). $$\n",
    "\n",
    "この時、$w', b'$は$\\frac{\\partial }{\\partial w} f(w, b), \\frac{\\partial }{\\partial w} f(w, b)$の値が0になると変化しないことがわかる。\n",
    "したがって、$\\frac{\\partial }{\\partial w} f(w, b) = 0, \\frac{\\partial }{\\partial w} f(w, b) = 0$になった時の$w', b'$の値が収束した値であり、ロスを最小化する$w,b$ということになる。\n",
    "\n",
    "\n",
    "### さらに具体的に解くと、、\n",
    "\n",
    "まずロス関数（MSE）の式を展開してみる。\n",
    "\n",
    "$$ \\frac{1}{N} \\sum_{(x,y) \\in D} (y - prediction(x))^2 $$\n",
    "$$ = \\frac{1}{N} \\sum_{(x,y) \\in D} (y - (b + wx))^2　$$\n",
    "$$ = \\frac{1}{N} \\sum_{(x,y) \\in D} (y^2 + b^2 + w^2x^2 - 2yb -2ywx  + 2bwx ). $$\n",
    "\n",
    "最初にwで偏微分することを考える。\n",
    "\n",
    "$$ \\frac{\\partial }{\\partial w} \\frac{1}{N} \\sum_{(x,y) \\in D} (y^2 + b^2 + w^2x^2 - 2yb -2ywx  + 2bwx ) = \\frac{1}{N} \\sum_{(x,y) \\in D} (2wx^2 -2yx  + 2bx ).$$\n",
    "\n",
    "したがって、$w'$は以下のように表せる。\n",
    "\n",
    "$$w' = w - \\alpha　\\frac{1}{N} \\sum_{(x,y) \\in D} (2wx^2 - 2yx  + 2bx ).$$\n",
    "\n",
    "同様に$b$で微分すると、\n",
    "\n",
    "$$ \\frac{\\partial }{\\partial b} \\frac{1}{N} \\sum_{(x,y) \\in D} (y^2 + b^2 + w^2x^2 - 2yb -2ywx  + 2bwx ) = \\frac{1}{N} \\sum_{(x,y) \\in D} (2b - 2y + 2wx ).$$\n",
    "\n",
    "したがって、\n",
    "$$b' = b - \\alpha \\frac{1}{N} \\sum_{(x,y) \\in D} (2b - 2y + 2wx ).$$\n",
    "\n",
    "具体的には上記の式を用いて次の点を求め、それを繰り返すことでロスが最小になるような$w, b$の値を求める。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
